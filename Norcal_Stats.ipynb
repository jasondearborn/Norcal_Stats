{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE THE STATS RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "season_map_df = pd.DataFrame({\n",
    "    'Season': [3, 4, 6, 9, 12, 15, 18, 20, 22, 24, 26, 27, 28],\n",
    "    'Season Year': [\n",
    "        '2010-11', '2011-12', '2012-13', '2013-14', '2014-15',\n",
    "        '2015-16', '2016-17', '2017-18', '2018-19', '2019-20',\n",
    "        '2020-21', '2021-22', '2022-23'\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://stats.caha.timetoscore.com/display-stats?league=3\"\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the season map\n",
    "for index, row in season_map_df.iterrows():\n",
    "    season_year = row['Season Year']\n",
    "    season_number = row['Season']\n",
    "    season_url = f\"{base_url}&season={season_number}\"\n",
    "    \n",
    "    # Fetch the content of the season URL\n",
    "    response = requests.get(season_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all the links on the page\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        # Initialize lists to store schedule names and division player stats URLs\n",
    "        schedule_names = []\n",
    "        division_player_stats_urls = []\n",
    "        \n",
    "        # Extract Schedule and Division Player Stats URLs\n",
    "        for link in links:\n",
    "            if 'Division Player Stats' in link.get_text():\n",
    "                division_player_stats_url = urljoin(base_url, link['href'])\n",
    "                division_player_stats_urls.append(division_player_stats_url)\n",
    "            elif 'Schedule' in link.get_text() and 'Norcal Schedule' not in link.get_text():\n",
    "                schedule_name = link.get_text()\n",
    "                schedule_names.append(schedule_name)\n",
    "        \n",
    "        # Append data to the list\n",
    "        data.extend([\n",
    "            {\n",
    "                'Season Name': season_year,\n",
    "                'Division': schedule_name.replace(' Schedule', '').strip(),\n",
    "                'Division Player Stats URL': division_player_stats_url\n",
    "            }\n",
    "            for schedule_name, division_player_stats_url in zip(schedule_names, division_player_stats_urls)\n",
    "        ])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "division_list_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETRIEVE PLAYER STATS FROM ALL SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_division_stats(url, season_name, division):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if table:\n",
    "            division_stats_df = pd.read_html(str(table), header=0)[0]\n",
    "            division_stats_df.insert(0, 'Season Name', season_name)\n",
    "            division_stats_df.insert(1, 'Division', division)\n",
    "            return division_stats_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the division list\n",
    "for index, row in division_list_df.iterrows():\n",
    "    season_name = row['Season Name']\n",
    "    division = row['Division']\n",
    "    division_player_stats_url = row['Division Player Stats URL']\n",
    "    \n",
    "    division_stats_df = scrape_division_stats(division_player_stats_url, season_name, division)\n",
    "    \n",
    "    if division_stats_df is not None:\n",
    "        data.append(division_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Player Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the list if there's any data\n",
    "if data:\n",
    "    result_df = pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicate lines\n",
    "    result_df.drop_duplicates(inplace=True)\n",
    " \n",
    "    # Define the new column names\n",
    "    new_column_names = ['Season Name', 'Division', 'Name', '#', 'Team', 'GP', 'Goals', 'Ass.', 'Hat', 'Min', 'Pts/Game', 'Pts']\n",
    "\n",
    "    # Update the column names with the new defined names\n",
    "    result_df.columns = new_column_names\n",
    "\n",
    "    # Remove any redundant rows where the \"Name\" column has the value \"Name\", starting from the second row\n",
    "    result_df = result_df.loc[(result_df['Name'] != 'Name') | (result_df.index == 1)]\n",
    "\n",
    "    # Write the cleaned DataFrame to a CSV file\n",
    "    result_df.to_csv('norcal_player_stats.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No valid data found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETRIEVE GOALIE STATS FROM ALL SEASONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_goalie_stats(url, season_name, division):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        if len(tables) > 1:\n",
    "            goalie_stats_df = pd.read_html(str(tables[1]), header=0)[0]\n",
    "            goalie_stats_df.insert(0, 'Season Name', season_name)\n",
    "            goalie_stats_df.insert(1, 'Division', division)\n",
    "            return goalie_stats_df\n",
    "    return None\n",
    "\n",
    "# Initialize a list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Iterate through the division list and scrape goalie stats\n",
    "for index, row in division_list_df.iterrows():\n",
    "    season_name = row['Season Name']\n",
    "    division = row['Division']\n",
    "    division_player_stats_url = row['Division Player Stats URL']\n",
    "    goalie_stats_df = scrape_goalie_stats(division_player_stats_url, season_name, division)\n",
    "    if goalie_stats_df is not None:\n",
    "        data.append(goalie_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Goalie Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and clean the scraped data\n",
    "if data:\n",
    "    goalie_stats_df = pd.concat(data, ignore_index=True)\n",
    "    goalie_stats_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Define the new column names\n",
    "    new_column_names = ['Season Name', 'Division', 'Name', 'Team', 'GP', 'Shots', 'GA', 'GAA', 'Save %', 'SO']\n",
    "\n",
    "    # Update the column names with the new defined names\n",
    "    goalie_stats_df.columns = new_column_names\n",
    "\n",
    "    # Remove any redundant rows where the \"Name\" column has the value \"Name\", starting from the second row\n",
    "    goalie_stats_df = goalie_stats_df.loc[(goalie_stats_df['Name'] != 'Name') | (goalie_stats_df.index == 1)]\n",
    "\n",
    "    # Write the cleaned DataFrame to a CSV file\n",
    "    goalie_stats_df.to_csv('norcal_goalie_stats.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No valid data found. Skipping writing to norcal_goalie_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
