{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_url is https://stats.caha.timetoscore.com/display-stats?league=3\n",
    "read in season_map.csv\n",
    "generate a list of URLs based on each season. For example if season is 3 then the url will be https://stats.caha.timetoscore.com/display-stats?league=3&season=3\n",
    "save to a DataFrame with the cooresponding season year\n",
    "\n",
    "Search each page for a list of Schedule URLs and Division Player Stats URLs. Ignore the \"Norcal Schedule\" link on each page. \n",
    "\n",
    "dataframe reformatted to be:\n",
    "Season Name, Schedule Name, Division Player Stats URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdearborn/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Read the season_map.csv file\n",
    "season_map_df = pd.read_csv('season_map.csv')\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://stats.caha.timetoscore.com/display-stats?league=3\"\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the season map\n",
    "for index, row in season_map_df.iterrows():\n",
    "    season_year = row['Season Year']\n",
    "    season_number = row['Season']\n",
    "    season_url = f\"{base_url}&season={season_number}\"\n",
    "    \n",
    "    # Fetch the content of the season URL\n",
    "    response = requests.get(season_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all the links on the page\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        # Initialize lists to store schedule names and division player stats URLs\n",
    "        schedule_names = []\n",
    "        division_player_stats_urls = []\n",
    "        \n",
    "        # Extract Schedule and Division Player Stats URLs\n",
    "        for link in links:\n",
    "            if 'Division Player Stats' in link.get_text():\n",
    "                division_player_stats_url = urljoin(base_url, link['href'])\n",
    "                division_player_stats_urls.append(division_player_stats_url)\n",
    "            elif 'Schedule' in link.get_text() and 'Norcal Schedule' not in link.get_text():\n",
    "                schedule_name = link.get_text()\n",
    "                schedule_names.append(schedule_name)\n",
    "        \n",
    "        # Append data to the list\n",
    "        data.extend([\n",
    "            {\n",
    "                'Season Name': season_year,\n",
    "                'Division': schedule_name.replace(' Schedule', '').strip(),\n",
    "                'Division Player Stats URL': division_player_stats_url\n",
    "            }\n",
    "            for schedule_name, division_player_stats_url in zip(schedule_names, division_player_stats_urls)\n",
    "        ])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "result_df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame written to division_list.csv\n"
     ]
    }
   ],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "result_df.to_csv('division_list.csv', index=False)\n",
    "\n",
    "print(\"DataFrame written to division_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the first table in the Division Player Stats URL and write it to a new dataframe\n",
    "create a CSV from the table. The first column should be Season Name, second is Division, and then append the data from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame written to norcal_player_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_division_stats(url, season_name, division):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if table:\n",
    "            division_stats_df = pd.read_html(str(table), header=0)[0]\n",
    "            division_stats_df.insert(0, 'Season Name', season_name)\n",
    "            division_stats_df.insert(1, 'Division', division)\n",
    "            return division_stats_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Read division_list.csv\n",
    "division_list_df = pd.read_csv('division_list.csv')\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the division list\n",
    "for index, row in division_list_df.iterrows():\n",
    "    season_name = row['Season Name']\n",
    "    division = row['Division']\n",
    "    division_player_stats_url = row['Division Player Stats URL']\n",
    "    \n",
    "    division_stats_df = scrape_division_stats(division_player_stats_url, season_name, division)\n",
    "    \n",
    "    if division_stats_df is not None:\n",
    "        data.append(division_stats_df)\n",
    "\n",
    "# Concatenate all DataFrames in the list if there's any data\n",
    "if data:\n",
    "    result_df = pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicate lines\n",
    "    result_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Write the DataFrame to a CSV file named norcal_player_stats.csv\n",
    "    result_df.to_csv('norcal_player_stats.csv', index=False)\n",
    "    \n",
    "    print(\"DataFrame written to norcal_player_stats.csv\")\n",
    "else:\n",
    "    print(\"No valid data found. Skipping writing to norcal_player_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape goalie stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame written to norcal_goalie_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_goalie_stats(url, season_name, division):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tables = soup.find_all('table')\n",
    "        \n",
    "        if len(tables) > 1:\n",
    "            goalie_stats_df = pd.read_html(str(tables[1]), header=0)[0]\n",
    "            goalie_stats_df.insert(0, 'Season Name', season_name)\n",
    "            goalie_stats_df.insert(1, 'Division', division)\n",
    "            return goalie_stats_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Read division_list.csv\n",
    "division_list_df = pd.read_csv('division_list.csv')\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the division list\n",
    "for index, row in division_list_df.iterrows():\n",
    "    season_name = row['Season Name']\n",
    "    division = row['Division']\n",
    "    division_player_stats_url = row['Division Player Stats URL']\n",
    "    \n",
    "    goalie_stats_df = scrape_goalie_stats(division_player_stats_url, season_name, division)\n",
    "    \n",
    "    if goalie_stats_df is not None:\n",
    "        data.append(goalie_stats_df)\n",
    "\n",
    "# Concatenate all DataFrames in the list if there's any data\n",
    "if data:\n",
    "    result_df = pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicate lines\n",
    "    result_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Write the DataFrame to a CSV file named norcal_goalie_stats.csv\n",
    "    result_df.to_csv('norcal_goalie_stats.csv', index=False)\n",
    "    \n",
    "    print(\"DataFrame written to norcal_goalie_stats.csv\")\n",
    "else:\n",
    "    print(\"No valid data found. Skipping writing to norcal_goalie_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Player Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norcal_player_stats.csv cleaned and updated\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read norcal_player_stats.csv\n",
    "player_stats_df = pd.read_csv('norcal_player_stats.csv')\n",
    "\n",
    "# Define the new column names\n",
    "new_column_names = ['Season Name', 'Division', 'Name', '#', 'Team', 'GP', 'Goals', 'Ass.', 'Hat', 'Min', 'Pts/Game', 'Pts']\n",
    "\n",
    "# Replace the column names\n",
    "player_stats_df.columns = new_column_names\n",
    "\n",
    "# Remove any rows where the \"Name\" column has the value \"Name\", starting from the second row\n",
    "player_stats_df = player_stats_df.loc[(player_stats_df['Name'] != 'Name') | (player_stats_df.index == 1)]\n",
    "\n",
    "# Write the cleaned DataFrame back to norcal_player_stats.csv\n",
    "player_stats_df.to_csv('norcal_player_stats.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Goalie Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read norcal_player_stats.csv\n",
    "goalie_stats_df = pd.read_csv('norcal_goalie_stats.csv')\n",
    "\n",
    "# Define the new column names\n",
    "new_column_names = ['Season Name', 'Division', 'Name', 'Team', 'GP', 'Shots', 'GA', 'GAA', 'Save %', 'SO']\n",
    "\n",
    "# Replace the column names\n",
    "goalie_stats_df.columns = new_column_names\n",
    "\n",
    "# Remove any rows where the \"Name\" column has the value \"Name\", starting from the second row\n",
    "goalie_stats_df = goalie_stats_df.loc[(goalie_stats_df['Name'] != 'Name') | (goalie_stats_df.index == 1)]\n",
    "\n",
    "# Write the cleaned DataFrame back to norcal_goalie_stats.csv\n",
    "goalie_stats_df.to_csv('norcal_goalie_stats.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
